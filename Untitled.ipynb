{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aabf1a20-94fb-428e-a4f9-f5a3e4ce0447",
   "metadata": {},
   "source": [
    "# I) - prétraitement des données "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b515435-1039-4b3d-999f-20ac5fcff714",
   "metadata": {},
   "source": [
    "> ## 1) - démarrer une session Spark  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15693d9f-f9ee-479f-b342-f2f44ebabd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@6c54805a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6c54805a"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f05d9-2fbd-4e6b-86df-f2961e203229",
   "metadata": {},
   "source": [
    "> ## 2) - importation  des données  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23d5c03-938a-49ce-957a-e291ab89c985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file = flights20170102.json\n",
       "df = [_id: string, arrdelay: double ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[_id: string, arrdelay: double ... 10 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var file = \"flights20170102.json\"\n",
    "val  df=spark.read.format(\"json\").option(\"inferSchema\",\"true\").load(file)\n",
    "df.createOrReplaceTempView(\"flights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40058bfc-d9b5-4de7-9fee-d768e37a5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+-----+----+------+\n",
      "|                 _id|arrdelay|carrier|crsarrtime|crsdephour|crsdeptime|crselapsedtime|depdelay|dest| dist|dofW|origin|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+-----+----+------+\n",
      "|AA_2017-01-01_ATL...|     0.0|     AA|      1912|        17|      1700|         132.0|     0.0| LGA|762.0|   7|   ATL|\n",
      "|AA_2017-01-01_LGA...|     0.0|     AA|      1620|        13|      1343|         157.0|     0.0| ATL|762.0|   7|   LGA|\n",
      "|AA_2017-01-01_MIA...|    10.0|     AA|      1137|         9|       939|         118.0|     0.0| ATL|594.0|   7|   MIA|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+-----+----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ce171-9df1-4bd1-b080-7841f1496340",
   "metadata": {},
   "source": [
    "> # 3) -  opérations de sélection sur des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac2718c-8d8b-4a72-8c55-a082020964c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|                 _id|arrdelay|carrier|crsarrtime|crsdephour|crsdeptime|crselapsedtime|depdelay|dest|  dist|dofW|origin|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|AA_2017-01-01_ORD...|     3.0|     AA|      1145|        10|      1005|         160.0|     5.0| DEN| 888.0|   7|   ORD|\n",
      "|AA_2017-01-01_MIA...|     0.0|     AA|      1231|        10|      1005|         206.0|     4.0| ORD|1197.0|   7|   MIA|\n",
      "|DL_2017-01-01_ORD...|     0.0|     DL|      1300|        10|      1000|         120.0|     2.0| ATL| 606.0|   7|   ORD|\n",
      "|DL_2017-01-01_DEN...|     0.0|     DL|      1524|        10|      1035|         169.0|     4.0| ATL|1199.0|   7|   DEN|\n",
      "|DL_2017-01-01_DEN...|     0.0|     DL|      1624|        10|      1045|         219.0|     0.0| LGA|1620.0|   7|   DEN|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "df.filter(col(\"crsdephour\") === 10).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7a87a7-0356-4e93-9bb5-14bfe06efc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|                 _id|arrdelay|carrier|crsarrtime|crsdephour|crsdeptime|crselapsedtime|depdelay|dest|  dist|dofW|origin|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|DL_2017-01-01_ATL...|     8.0|     DL|      1205|        10|      1005|         120.0|    20.0| MIA| 594.0|   7|   ATL|\n",
      "|DL_2017-01-01_ATL...|     0.0|     DL|      1218|        10|      1010|         128.0|    15.0| EWR| 746.0|   7|   ATL|\n",
      "|UA_2017-01-01_DEN...|    24.0|     UA|      1343|        10|      1020|         143.0|    28.0| ORD| 888.0|   7|   DEN|\n",
      "|UA_2017-01-01_ORD...|     7.0|     UA|      1349|        10|      1040|         129.0|    22.0| EWR| 719.0|   7|   ORD|\n",
      "|UA_2017-01-01_EWR...|    55.0|     UA|      1319|        10|      1042|         157.0|    70.0| ATL| 746.0|   7|   EWR|\n",
      "|UA_2017-01-01_ORD...|    13.0|     UA|      1332|        10|      1045|         287.0|    37.0| SFO|1846.0|   7|   ORD|\n",
      "|AA_2017-01-01_ATL...|   141.0|     AA|      1106|        10|       959|         127.0|   137.0| ORD| 606.0|   7|   ATL|\n",
      "|AA_2017-01-02_ORD...|   319.0|     AA|      1314|        10|      1034|         280.0|   294.0| SFO|1846.0|   1|   ORD|\n",
      "|WN_2017-01-02_SFO...|   118.0|     WN|      1315|        10|       950|         145.0|   115.0| DEN| 967.0|   1|   SFO|\n",
      "|AA_2017-01-02_BOS...|    59.0|     AA|      1215|        10|      1015|         180.0|    45.0| ORD| 867.0|   1|   BOS|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"crsdephour\") === 10 && col(\"depdelay\") >= 10).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477fbda3-f459-4518-bc08-286ff84c9e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|carrier|count|\n",
      "+-------+-----+\n",
      "|     UA|18873|\n",
      "|     AA|10031|\n",
      "|     DL|10055|\n",
      "|     WN| 2389|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"carrier\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3230d3ac-4497-4aaa-9b66-6e5972c3c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-------------+---------------+\n",
      "|carrier|max(depdelay)|min(depdelay)|sum(crsdeptime)|\n",
      "+-------+-------------+-------------+---------------+\n",
      "|     UA|       1138.0|          0.0|       24267859|\n",
      "|     AA|       1440.0|          0.0|       13226373|\n",
      "|     DL|       1185.0|          0.0|       13318833|\n",
      "|     WN|        375.0|          0.0|        3104545|\n",
      "+-------+-------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"carrier\").agg(\"depdelay\" -> \"max\", \"depdelay\" -> \"min\" , \"crsdeptime\" -> \"sum\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a138127-a8ad-40a7-bbce-df38cfdaca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|dest|count|\n",
      "+----+-----+\n",
      "| SFO|  926|\n",
      "| EWR|  699|\n",
      "| ORD|  670|\n",
      "| LGA|  647|\n",
      "| ATL|  616|\n",
      "| DEN|  462|\n",
      "| MIA|  432|\n",
      "| BOS|  393|\n",
      "| IAH|  375|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resultDF = [dest: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dest: string, count: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.desc\n",
    "val resultDF = df\n",
    "  .filter($\"depdelay\" > 40 || $\"arrdelay\" > 40)\n",
    "  .groupBy(\"dest\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be97c04d-1bf0-4f38-936f-bb42a131410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|                 _id|arrdelay|carrier|crsarrtime|crsdephour|crsdeptime|crselapsedtime|depdelay|dest|  dist|dofW|origin|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "|AA_2017-02-22_SFO...|  1442.0|     AA|      1411|         8|       800|         251.0|  1440.0| ORD|1846.0|   3|   SFO|\n",
      "|DL_2017-01-07_BOS...|  1158.0|     DL|      2024|        17|      1715|         189.0|  1185.0| ATL| 946.0|   6|   BOS|\n",
      "|UA_2017-02-23_DEN...|  1139.0|     UA|      1824|        12|      1244|         220.0|  1138.0| EWR|1605.0|   4|   DEN|\n",
      "|DL_2017-01-22_ORD...|  1090.0|     DL|      2240|        19|      1935|         125.0|  1087.0| ATL| 606.0|   7|   ORD|\n",
      "|UA_2017-01-02_MIA...|  1090.0|     UA|      2340|        20|      2044|         176.0|  1072.0| EWR|1085.0|   1|   MIA|\n",
      "+--------------------+--------+-------+----------+----------+----------+--------------+--------+----+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.cache\n",
    "df.createOrReplaceTempView(\"flights\")\n",
    "spark.catalog.cacheTable(\"flights\")\n",
    "spark.sql(\"select * from flights where  depdelay >  40 order  by depdelay desc  limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1dec756-e298-409a-8ded-6df205d41a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+--------+----------+------+\n",
      "|carrier|origin|dest|depdelay|crsdephour|  dist|\n",
      "+-------+------+----+--------+----------+------+\n",
      "|     AA|   SFO| ORD|  1440.0|         8|1846.0|\n",
      "|     DL|   BOS| ATL|  1185.0|        17| 946.0|\n",
      "|     UA|   DEN| EWR|  1138.0|        12|1605.0|\n",
      "|     DL|   ORD| ATL|  1087.0|        19| 606.0|\n",
      "|     UA|   MIA| EWR|  1072.0|        20|1085.0|\n",
      "|     DL|   DEN| ATL|  1034.0|        13|1199.0|\n",
      "|     DL|   DEN| LGA|  1004.0|        16|1620.0|\n",
      "|     UA|   BOS| ORD|   972.0|        18| 867.0|\n",
      "|     UA|   MIA| SFO|   964.0|        18|2585.0|\n",
      "|     DL|   DEN| LGA|   952.0|        16|1620.0|\n",
      "|     UA|   MIA| SFO|   925.0|        18|2585.0|\n",
      "|     DL|   BOS| ATL|   893.0|        11| 946.0|\n",
      "|     DL|   BOS| LGA|   869.0|         7| 184.0|\n",
      "|     UA|   ATL| EWR|   809.0|        20| 746.0|\n",
      "|     UA|   MIA| IAH|   779.0|        11| 964.0|\n",
      "|     UA|   ORD| LGA|   756.0|        21| 733.0|\n",
      "|     DL|   ORD| ATL|   749.0|        11| 606.0|\n",
      "|     AA|   SFO| MIA|   727.0|        21|2585.0|\n",
      "|     UA|   MIA| EWR|   687.0|         8|1085.0|\n",
      "|     AA|   MIA| SFO|   668.0|        21|2585.0|\n",
      "+-------+------+----+--------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select($\"carrier\" ,$\"origin\",$\"dest\",$\"depdelay\",$\"crsdephour\" , $\"dist\")\n",
    "    .filter($\"depdelay\" > 40 ).orderBy(desc(\"depdelay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a177de-650d-47eb-9c9b-4b1f30fd855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|carrier|     avg(depdelay)|\n",
      "+-------+------------------+\n",
      "|     UA|17.477878450696764|\n",
      "|     AA| 10.45768118831622|\n",
      "|     DL|15.316061660865241|\n",
      "|     WN|13.491000418585182|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "df.groupBy(\"carrier\").agg(avg(\"depdelay\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eea9ead-8322-430e-b268-a28479c40086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|carrier|     avg(depdelay)|\n",
      "+-------+------------------+\n",
      "|     UA|17.477878450696764|\n",
      "|     AA| 10.45768118831622|\n",
      "|     DL|15.316061660865241|\n",
      "|     WN|13.491000418585182|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resultDF = [carrier: string, avg(depdelay): double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[carrier: string, avg(depdelay): double]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"flights\")\n",
    "val resultDF = spark.sql(\"SELECT carrier, AVG(depdelay) FROM flights GROUP BY carrier\")\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f4a2491-e366-4775-bd7e-a004b90d30d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Magic scala failed to execute with error: \n",
       "<console>:32: error: not found: value vegas\n",
       "       import vegas._\n",
       "              ^\n",
       "<console>:33: error: not found: value vegas\n",
       "       import vegas.render.WindowRenderer._\n",
       "              ^\n",
       "<console>:34: error: not found: value vegas\n",
       "       import vegas.sparkExt._\n",
       "              ^\n",
       "<console>:37: error: not found: value Vegas\n",
       "       val plot = Vegas(\"Average Departure Delay by Carrier\")\n",
       "                  ^\n",
       "<console>:39: error: not found: value axis\n",
       "         .encodeX(\"carrier:N\", axis = Axis(title = \"Carrier\"))\n",
       "                               ^\n",
       "<console>:40: error: not found: value axis\n",
       "         .encodeY(\"AVG(depdelay):Q\", axis = Axis(title = \"Average Departure Delay\"))\n",
       "                                     ^\n",
       "<console>:41: error: not found: value Bar\n",
       "         .mark(Bar)\n",
       "               ^\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%scala\n",
    "import vegas._\n",
    "import vegas.render.WindowRenderer._\n",
    "import vegas.sparkExt._\n",
    "\n",
    "// Create a bar chart using Vegas\n",
    "val plot = Vegas(\"Average Departure Delay by Carrier\")\n",
    "  .withDataFrame(resultDF)\n",
    "  .encodeX(\"carrier:N\", axis = Axis(title = \"Carrier\"))\n",
    "  .encodeY(\"AVG(depdelay):Q\", axis = Axis(title = \"Average Departure Delay\"))\n",
    "  .mark(Bar)\n",
    "\n",
    "// Display the chart\n",
    "plot.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbfe26-2496-4a05-92c3-801f8debd221",
   "metadata": {},
   "source": [
    "# II ) - classification : régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c485380-193d-45d8-a47f-4ccc38cf74ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+-----+------+\n",
      "|origin                                                                    |width|height|\n",
      "+--------------------------------------------------------------------------+-----+------+\n",
      "|file:///opt/spark/data/mllib/images/origin/kittens/54893.jpg              |300  |311   |\n",
      "|file:///opt/spark/data/mllib/images/origin/kittens/DP802813.jpg           |199  |313   |\n",
      "|file:///opt/spark/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg|300  |200   |\n",
      "|file:///opt/spark/data/mllib/images/origin/kittens/DP153539.jpg           |300  |296   |\n",
      "+--------------------------------------------------------------------------+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [image: struct<origin: string, height: int ... 4 more fields>]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[image: struct<origin: string, height: int ... 4 more fields>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"/opt/spark/data/mllib/images/origin/kittens\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd1c8b9-2e5b-4fca-8a3d-c9b78f5b9f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "|  1.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[151,152,153...|\n",
      "|  0.0|(780,[129,130,131...|\n",
      "|  1.0|(780,[158,159,160...|\n",
      "|  1.0|(780,[99,100,101,...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[127,128,129...|\n",
      "|  1.0|(780,[154,155,156...|\n",
      "|  0.0|(780,[153,154,155...|\n",
      "|  0.0|(780,[151,152,153...|\n",
      "|  1.0|(780,[129,130,131...|\n",
      "|  0.0|(780,[154,155,156...|\n",
      "|  1.0|(780,[150,151,152...|\n",
      "|  0.0|(780,[124,125,126...|\n",
      "|  0.0|(780,[152,153,154...|\n",
      "|  1.0|(780,[97,98,99,12...|\n",
      "|  1.0|(780,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1 = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.read.format(\"libsvm\").option(\"numFeatures\", \"780\").load(\"/opt/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84e0e84-3ba2-4b2e-a7b1-74b7d448f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a9be5-cc93-4d2d-be08-f17175de4059",
   "metadata": {},
   "source": [
    "# Création du dataset d’apprentissage (training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca242d6-9609-476e-a0df-11850ea231fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.createDataFrame(Seq(\n",
    "(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "(0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "(1.0, Vectors.dense(0.0, 1.2, -0.5))\n",
    ")).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4ebe9-9027-4178-9909-7f33fd93eab9",
   "metadata": {},
   "source": [
    "# 3 -  Création d’une instance d’un algorithme d’apprentissage (ex : Logistic resgression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a356b44-8353-42fa-865e-cfadd73525d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr = logreg_fd29f7425c7b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "logreg_fd29f7425c7b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr  = new LogisticRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942e4e6-f209-4b60-b1e5-b43733a735e6",
   "metadata": {},
   "source": [
    "# Afficher les paramètres par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55720c84-b371-4a24-93b2-3424e6f05153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxBlockSizeInMB: Maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(s\"LogisticRegression parameters:\\n ${lr.explainParams()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf08fcb-b92e-4432-a201-a067ae0fee13",
   "metadata": {},
   "source": [
    "# 5 - Modifier les paramètres de la régression suivant le besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77d5fac-06c9-4201-8b23-2ce88709dcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logreg_fd29f7425c7b"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setMaxIter(10)\n",
    ".setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3993c-fbc7-457f-851b-8c6a48545237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98300da7-52c7-441b-9203-d97695f11bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model1 = LogisticRegressionModel: uid=logreg_fd29f7425c7b, numClasses=2, numFeatures=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=logreg_fd29f7425c7b, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb58d0c-b139-4028-b833-6ed61df2fc67",
   "metadata": {},
   "source": [
    "# fficher les paramètres utilisés lors de cette apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d9624a-cb99-4aec-a581-7038000aecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: {\n",
      "\tlogreg_fd29f7425c7b-aggregationDepth: 2,\n",
      "\tlogreg_fd29f7425c7b-elasticNetParam: 0.0,\n",
      "\tlogreg_fd29f7425c7b-family: auto,\n",
      "\tlogreg_fd29f7425c7b-featuresCol: features,\n",
      "\tlogreg_fd29f7425c7b-fitIntercept: true,\n",
      "\tlogreg_fd29f7425c7b-labelCol: label,\n",
      "\tlogreg_fd29f7425c7b-maxBlockSizeInMB: 0.0,\n",
      "\tlogreg_fd29f7425c7b-maxIter: 10,\n",
      "\tlogreg_fd29f7425c7b-predictionCol: prediction,\n",
      "\tlogreg_fd29f7425c7b-probabilityCol: probability,\n",
      "\tlogreg_fd29f7425c7b-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_fd29f7425c7b-regParam: 0.01,\n",
      "\tlogreg_fd29f7425c7b-standardization: true,\n",
      "\tlogreg_fd29f7425c7b-threshold: 0.5,\n",
      "\tlogreg_fd29f7425c7b-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(s\"Model 1 was fit using parameters: ${model1.parent.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb9820-6a8a-4097-8851-91fa1e947714",
   "metadata": {},
   "source": [
    "# les paramètres peuvent être spécifiés en utilisant ParamMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475328e3-9d92-41a6-82b7-a3d5b2e9b85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tlogreg_fd29f7425c7b-maxIter: 30,\n",
       "\tlogreg_fd29f7425c7b-regParam: 0.1,\n",
       "\tlogreg_fd29f7425c7b-threshold: 0.55\n",
       "}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "val paramMap = ParamMap(lr.maxIter -> 20)\n",
    ".put(lr.maxIter, 30) // spécifie 1 Param. qui écrase la valeur originale\n",
    ".put(lr.regParam -> 0.1, lr.threshold -> 0.55) // spécifie plusieurs Params."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b22f8a-86cd-46f2-9db9-a9edf8abd38b",
   "metadata": {},
   "source": [
    "# Créer un nouveau modèle avec les nouveaux paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5a763f-c065-4b7c-a92c-277342e93af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMap2 = \n",
       "paramMapCombined = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tlogreg_fd29f7425c7b-probabilityCol: myProbability\n",
       "}\n",
       "{\n",
       "\tlogreg_fd29f7425c7b-maxIter: 30,\n",
       "\tlogreg_fd29f7425c7b-probabilityCol: myProbability,\n",
       "\tlogreg_fd29f7425c7b-regParam: 0.1,\n",
       "\tlogreg_fd29f7425c7b-threshold: 0.55\n",
       "}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "val paramMap2 = ParamMap(lr.probabilityCol -> \"myProbability\") \n",
    "val paramMapCombined = paramMap ++ paramMap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc84ef4-cb59-417a-8c37-a607fac35a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fit using parameters: {\n",
      "\tlogreg_fd29f7425c7b-aggregationDepth: 2,\n",
      "\tlogreg_fd29f7425c7b-elasticNetParam: 0.0,\n",
      "\tlogreg_fd29f7425c7b-family: auto,\n",
      "\tlogreg_fd29f7425c7b-featuresCol: features,\n",
      "\tlogreg_fd29f7425c7b-fitIntercept: true,\n",
      "\tlogreg_fd29f7425c7b-labelCol: label,\n",
      "\tlogreg_fd29f7425c7b-maxBlockSizeInMB: 0.0,\n",
      "\tlogreg_fd29f7425c7b-maxIter: 30,\n",
      "\tlogreg_fd29f7425c7b-predictionCol: prediction,\n",
      "\tlogreg_fd29f7425c7b-probabilityCol: myProbability,\n",
      "\tlogreg_fd29f7425c7b-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_fd29f7425c7b-regParam: 0.1,\n",
      "\tlogreg_fd29f7425c7b-standardization: true,\n",
      "\tlogreg_fd29f7425c7b-threshold: 0.55,\n",
      "\tlogreg_fd29f7425c7b-tol: 1.0E-6\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model2 = LogisticRegressionModel: uid=logreg_fd29f7425c7b, numClasses=2, numFeatures=3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=logreg_fd29f7425c7b, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model2 = lr.fit(training, paramMapCombined)\n",
    "println(s\"Model 2 was fit using parameters: ${model2.parent.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5bdca-bfa1-4360-b3f5-93c2ce804a9f",
   "metadata": {},
   "source": [
    "# Préparer les données de tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac56ed10-7e43-4933-bb77-0a30ad9440f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq(\n",
    "(1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "(0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "(1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf2ee62-414e-4ef5-ac9d-04b024fc51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0|[-1.0,1.5,1.3]|\n",
      "|  0.0|[3.0,2.0,-0.1]|\n",
      "|  1.0|[0.0,2.2,-1.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc751ea2-1de1-4f84-8a3c-134562c46489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304993572542,0.9429269500642746], prediction=1.0\n",
      "([3.0,2.0,-0.1], 0.0) -> prob=[0.923852195644323,0.07614780435567703], prediction=0.0\n",
      "([0.0,2.2,-1.5], 1.0) -> prob=[0.10972780286187808,0.8902721971381219], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "model2.transform(test)\n",
    ".select(\"features\", \"label\", \"myProbability\", \"prediction\")\n",
    ".collect()\n",
    ".foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) => println(s\"($features, $label) -> prob=$prob, prediction=$prediction\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c74aa-f65a-453a-bec1-286dfa7ef15b",
   "metadata": {},
   "source": [
    "> #  pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24f370dc-7b05-4940-b7f5-1f4dd7dc3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ce1a3-51ef-410c-ad2b-54401dff4679",
   "metadata": {},
   "source": [
    " ## ---- 1 Préparer les données d’apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff0076a-ca04-485a-8b2b-78ded4efbf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training = [id: bigint, text: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string ... 1 more field]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training = spark.createDataFrame(Seq(\n",
    "(0L, \"a b c d e spark\", 1.0),\n",
    "(1L, \"b d\", 0.0),\n",
    "(2L, \"spark f g h\", 1.0),\n",
    "(3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "training.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e954f08-a744-465f-8f7f-47872f271a34",
   "metadata": {},
   "source": [
    "## 2 - Configurer le pipline qui consist en trois étapes : tokenizer, hashingTF, et lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c54328f3-5260-42b2-a4d7-d77e4022656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_a21d06ad0636\n",
       "hashingTF = HashingTF: uid=hashingTF_590afae9f13e, binary=false, numFeatures=1000\n",
       "lr = logreg_bcbbfc29cc2b\n",
       "pipeline = pipeline_cb135bfa098e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_cb135bfa098e"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer()\n",
    ".setInputCol(\"text\")\n",
    ".setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF()\n",
    ".setNumFeatures(1000)\n",
    ".setInputCol(tokenizer.getOutputCol)\n",
    ".setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression()\n",
    ".setMaxIter(10)\n",
    ".setRegParam(0.001)\n",
    "val pipeline = new Pipeline()\n",
    ".setStages(Array(tokenizer, hashingTF, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400ef0d-c522-46f4-aaaa-9039c8b9ac25",
   "metadata": {},
   "source": [
    "## 3 - Appliquer le pipeline sur les données d’apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4eaf8a2-ae96-4300-8700-53c83c8d5b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = pipeline_cb135bfa098e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_cb135bfa098e"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453d15d-3da8-4dc2-a015-bcf31d0270f1",
   "metadata": {},
   "source": [
    "## 4 - On peut à ce niveau persister le pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792dcfd5-06ca-407b-bcbc-a8716f57c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(\"./model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30765597-a44d-435b-a511-a28346b32fb5",
   "metadata": {},
   "source": [
    "## 5 - import le  modéle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24fbc128-128a-48e2-b50b-e41f5c79b71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel = pipeline_cb135bfa098e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_cb135bfa098e"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameModel = PipelineModel.load(\"./model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103dca0d-1fb9-4fce-be03-241225472943",
   "metadata": {},
   "source": [
    "## 6 - faire la prédiction sur les données de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf40603a-7491-44bb-a65d-f3908d3e5a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.6292098489668488,0.37079015103315116], prediction=0.0\n",
      "(5, l m n) --> prob=[0.9847700067623042,0.015229993237695805], prediction=0.0\n",
      "(6, spark hadoop spark) --> prob=[0.13412348342566116,0.8658765165743388], prediction=1.0\n",
      "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test = [id: bigint, text: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, text: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq(\n",
    "(4L, \"spark i j k\"),\n",
    "(5L, \"l m n\"),\n",
    "(6L, \"spark hadoop spark\"),\n",
    "(7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "model.transform(test)\n",
    ".select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    ".collect()\n",
    ".foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double)\n",
    "=> println(s\"($id, $text) --> prob=$prob, prediction=$prediction\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54e465e-7dec-440a-a42e-2ba69e794f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "778f2b7a-408a-4e94-84fb-276270c1ae34",
   "metadata": {},
   "source": [
    "> # Extracting, transforming and selectiong Features :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17ef55-5e3c-47ee-8de9-dadabc00ea30",
   "metadata": {},
   "source": [
    "## 1- mporter les fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "708bc10d-5035-400f-a918-2182c3dd511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b0f96-7cae-469f-a788-a2ba15b29768",
   "metadata": {},
   "source": [
    "## 2 - Transformer le texte en termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "485a2c41-7e04-4437-b184-2eee64dd7ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentenceData = [label: double, sentence: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[label: double, sentence: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sentenceData = spark.createDataFrame(Seq(\n",
    "(0.0, \"Hi I heard about Spark\"),\n",
    "(0.0, \"I wish Java could use case classes\"),\n",
    "(1.0, \"Logistic regression models are neat\")\n",
    ")).toDF(\"label\", \"sentence\")\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bdafae4-5114-4cdb-8d86-902a134f7b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_26d94a1f70d6\n",
       "wordsData = [label: double, sentence: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, sentence: string ... 1 more field]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d53b3-7bf8-4464-98ac-c5e8d0efec4e",
   "metadata": {},
   "source": [
    "## 3 - Transformer les termes en vecteurs de caractéristiques (features extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2982169f-a14a-45e3-b2ed-535a6b153c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashingTF = HashingTF: uid=hashingTF_a095f348c3b1, binary=false, numFeatures=20\n",
       "featurizedData = [label: double, sentence: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[label: double, sentence: string ... 2 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hashingTF = new HashingTF()\n",
    ".setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\n",
    "val featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac9381-674d-4f18-afb2-58e92cd7863b",
   "metadata": {},
   "source": [
    "## 4 - création du modèle à l’aide de l’estimateur IDF () qui implémente la méthode fit () qui prends\n",
    "les données comme entrée et produit le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42ed406a-a326-4f6e-91bc-ae35cbe0478e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf = idf_da8dc14a25e0\n",
       "idfModel = IDFModel: uid=idf_da8dc14a25e0, numDocs=3, numFeatures=20\n",
       "rescaledData = [label: double, sentence: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[6,8,13,16],[...|\n",
      "|  0.0|(20,[0,2,7,13,15,...|\n",
      "|  1.0|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[label: double, sentence: string ... 3 more fields]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c63a44-40b8-4031-96d6-93d354774afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
